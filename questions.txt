1. If we are factoring through e^{i\gamma C} gate then are we just doing matrix multiplication?
2. I guess my real question is how deep are we going into the factorization?
3. Are we going to use grid-stride loop for parallelization?















Notes:
1. Adding __global__ to the top of a function lets the CUDA C++ compiler know that this a function that runs on the GPU and can be called from CPU code. These __global__ functions are known as kernels. The code that runs on the GPU is often called the device code, while the code that runs on the CPU is host code.
2. To compute on the GPU, we need to allocate memory accessible by the GPU. To allocate data in unified memory, call cudaMallocManaged(), which returns a pointer that you can access from host (CPU) code or device (GPU) code. To free the data, just pass the pointer to cudaFree().
3. Once a kernel is launched, its important to tell the CPU to wait until the kernel is done before the CPU tries to access the results. This is because CUDA kernel launches don't block the called CPU thread. To sync the two, call cudaDeviceSynchronize() before checking the results.
4. To parallelize we look at the <<<a, b>>> syntax. The b parameter controls the number of threads in a thread block. CUDA GPUs run kernels using blocks of threads that are a multiple of 32 in size.
5. threadIdx.x is the index of the current thread within its block
6. blockDim.x contains the number of threads in the block
7. 
